{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 训练",
   "id": "18278cc59ea06741"
  },
  {
   "cell_type": "markdown",
   "id": "536112ff851a956a",
   "metadata": {},
   "source": "## 设置随机种子"
  },
  {
   "cell_type": "code",
   "id": "d9dd3966f65b1b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T09:18:06.756982Z",
     "start_time": "2025-02-14T09:18:06.582325Z"
    }
   },
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 1. 设置随机种子，保证实验可复现\n",
    "# -------------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 创建保存中间结果的目录\n",
    "import os\n",
    "\n",
    "os.makedirs(\"intermediates\", exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 106
  },
  {
   "cell_type": "markdown",
   "id": "478cdd0dfd5e99e",
   "metadata": {},
   "source": [
    "## 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "id": "22ef30d33f89387b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T09:18:06.801375Z",
     "start_time": "2025-02-14T09:18:06.799417Z"
    }
   },
   "source": [
    "config = {\n",
    "    \"data_dir\": \"/data/Workspace/CIC-IoTDataset2023/bin-class\",  # 数据集目录\n",
    "    \"num_classes\": 2,  # 类别数\n",
    "    \"batch_size\": 128,  # 批量大小（根据显存实际情况调整）\n",
    "    \"lr\": 1e-4,  # 学习率\n",
    "    \"epochs\": 100,  # 总训练轮数\n",
    "    \"model_name\": \"Conv2Former\",\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 107
  },
  {
   "cell_type": "markdown",
   "id": "691953a405c389f3",
   "metadata": {},
   "source": [
    "## 数据增强设置"
   ]
  },
  {
   "cell_type": "code",
   "id": "695a82d4c5b0d9c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T09:18:06.849465Z",
     "start_time": "2025-02-14T09:18:06.846886Z"
    }
   },
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    # transforms.RandomResizedCrop(64, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 108
  },
  {
   "cell_type": "markdown",
   "id": "257f029541c8be7f",
   "metadata": {},
   "source": [
    "## 数据集与 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "id": "cc6d0ccf5a78946e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T09:18:06.948684Z",
     "start_time": "2025-02-14T09:18:06.894671Z"
    }
   },
   "source": [
    "from MyDataset import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = CustomDataset(os.path.join(config[\"data_dir\"], \"train\"), transform=train_transform)\n",
    "valid_dataset = CustomDataset(os.path.join(config[\"data_dir\"], \"valid\"), transform=valid_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=8, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config[\"batch_size\"], shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 109
  },
  {
   "cell_type": "markdown",
   "id": "7dfcbda6ce2a52e3",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "id": "9699bf510ad237c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T09:18:07.069023Z",
     "start_time": "2025-02-14T09:18:06.954502Z"
    }
   },
   "source": [
    "# 这里 dims 与 depths 可根据实验调节，本示例给出较轻量配置\n",
    "from deepseek import Conv2Former\n",
    "\n",
    "model = Conv2Former(dims=[96, 192, 384, 768], depths=[3, 3, 9, 3], num_classes=config[\"num_classes\"])\n",
    "model = model.to(config[\"device\"])"
   ],
   "outputs": [],
   "execution_count": 110
  },
  {
   "cell_type": "markdown",
   "id": "afc320e3239f98df",
   "metadata": {},
   "source": [
    "## 损失函数、优化器与学习率调度器"
   ]
  },
  {
   "cell_type": "code",
   "id": "1a77d7f9471d0aa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T09:18:07.079170Z",
     "start_time": "2025-02-14T09:18:07.074466Z"
    }
   },
   "source": [
    "from torch import nn, optim\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "# 这里采用了类别权重，适用于类别不平衡场景\n",
    "class_weights = torch.tensor([7.0, 1.0])\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1, weight=class_weights.to(config[\"device\"]))\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"])\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, epochs=config[\"epochs\"], max_lr=1e-3,\n",
    "                                          total_steps=config[\"epochs\"] * len(train_loader),\n",
    "                                          pct_start=0.1, anneal_strategy=\"cos\")\n",
    "# 如果需要可尝试 OneCycleLR 或 Warmup 策略\n",
    "\n",
    "best_val_acc = 0.0\n",
    "scaler = GradScaler()  # 用于混合精度训练"
   ],
   "outputs": [],
   "execution_count": 111
  },
  {
   "cell_type": "markdown",
   "id": "36e51c008a97bb60",
   "metadata": {},
   "source": [
    "## 训练与验证函数"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T09:18:07.118701Z",
     "start_time": "2025-02-14T09:18:07.114739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "reports = []\n",
    "\n",
    "\n",
    "def train(epoch_num: int):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=f\"Train {epoch_num:>3}/{config['epochs']}\", leave=True, unit=\" batch\")\n",
    "    for images, labels in train_bar:\n",
    "        images = images.to(config[\"device\"])\n",
    "        labels = labels.to(config[\"device\"])\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        # 梯度裁剪，防止梯度爆炸\n",
    "        scaler.unscale_(optimizer)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "    return total_loss / len(train_dataset)\n",
    "\n",
    "\n",
    "def validate(epoch_num: int):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predicts = []\n",
    "    all_labels = []\n",
    "    valid_bar = tqdm(valid_loader, desc=f\"Valid {epoch_num:>3}/{config['epochs']}\", leave=True, unit=\" batch\")\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_bar:\n",
    "            images = images.to(config[\"device\"])\n",
    "            labels = labels.to(config[\"device\"])\n",
    "            # outputs = model(images)\n",
    "            # TTA: 原图 + 水平翻转\n",
    "            outputs = model(images) + model(torch.flip(images, dims=[-1]))\n",
    "            _, predicts = torch.max(outputs, 1)\n",
    "            correct += (predicts == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_predicts.extend(predicts.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            valid_bar.set_postfix(batch_acc=(predicts == labels).float().mean().item())\n",
    "        valid_acc = correct / total\n",
    "        reports.append((all_labels, all_predicts))\n",
    "    return valid_acc"
   ],
   "id": "4b6478c422cb70c9",
   "outputs": [],
   "execution_count": 112
  },
  {
   "cell_type": "markdown",
   "id": "9d7d8d9cf1a231d2",
   "metadata": {},
   "source": [
    "## 训练循环"
   ]
  },
  {
   "cell_type": "code",
   "id": "a4981df491121931",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-14T09:18:07.162586Z"
    }
   },
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('runs/conv2former_train')\n",
    "train_losses = []\n",
    "valid_accs = []\n",
    "best_model = None\n",
    "patience_counter = 0\n",
    "patience = 10\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    train_loss = train(epoch_num=epoch + 1)\n",
    "    train_losses.append(train_loss)\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "\n",
    "    valid_acc = validate(epoch_num=epoch + 1)\n",
    "    valid_accs.append(valid_acc)\n",
    "    writer.add_scalar('Accuracy/val', valid_acc, epoch)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if valid_acc > best_val_acc:\n",
    "        best_val_acc = valid_acc\n",
    "        best_model = model\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Val Acc: {valid_acc:.4f}\")\n",
    "    # 检查训练损失是否下降\n",
    "    if epoch > 0 and train_loss >= train_losses[epoch - 1]:\n",
    "        patience_counter += 1\n",
    "    else:\n",
    "        patience_counter = 0  # 训练损失下降时重置计数器\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Stopped at epoch {epoch + 1} due to no improvement in loss.\")\n",
    "        break\n",
    "\n",
    "writer.close()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train   1/100:   0%|          | 0/657 [00:00<?, ? batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "361eae495b9b4b7987b289149a8df6ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Valid   1/100:   0%|          | 0/147 [00:00<?, ? batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e8177273ba9485a8a354a43980b4ac5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.0905, Val Acc: 0.9263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train   2/100:   0%|          | 0/657 [00:00<?, ? batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6bf08cfaf21426fb548fc72b05e0ec2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Valid   2/100:   0%|          | 0/147 [00:00<?, ? batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a135de1a16c3480aba1d6eb24c3c76d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 0.0709, Val Acc: 0.9582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train   3/100:   0%|          | 0/657 [00:00<?, ? batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2eb5d22af18438a83406ce970defbfe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Valid   3/100:   0%|          | 0/147 [00:00<?, ? batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38dab058f78140e5941e2562c423d765"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 0.0662, Val Acc: 0.9695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train   4/100:   0%|          | 0/657 [00:00<?, ? batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e11ebccc6364f30a6e4daf8ba5200e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 保存最佳模型",
   "id": "ca9fa50806755fe7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H_%M\")\n",
    "torch.save(best_model.state_dict(), f\"intermediates/model_{timestamp}_{best_val_acc * 100:.2f}.pth\")"
   ],
   "id": "3257ec5321a3f396"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 保存数据",
   "id": "1d090bb5b97c6720"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T09:17:36.950517Z",
     "start_time": "2025-02-14T09:17:36.941374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "training_data = {'train_losses': train_losses, 'valid_accs': valid_accs, 'reports': reports}\n",
    "# 获取当前时间，格式为 'YYYYMMDD_HH_MM'\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H_%M\")\n",
    "file_name = f\"intermediates/training_data_{timestamp}.pkl\"\n",
    "latest_train_data = \"intermediates/training_data_latest.pkl\"\n",
    "# 保存数据到文件\n",
    "with open(file_name, \"wb\") as f, open(latest_train_data, \"wb\") as latest_file:\n",
    "    pickle.dump(training_data, f)\n",
    "    pickle.dump(training_data, latest_file)"
   ],
   "id": "83c8884ea867d6ab",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reports' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[105], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpickle\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m training_data \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_losses\u001B[39m\u001B[38;5;124m'\u001B[39m: train_losses, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalid_accs\u001B[39m\u001B[38;5;124m'\u001B[39m: valid_accs, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreports\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[43mreports\u001B[49m}\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# 获取当前时间，格式为 'YYYYMMDD_HH_MM'\u001B[39;00m\n\u001B[1;32m      5\u001B[0m timestamp \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mnow()\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mH_\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mM\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'reports' is not defined"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 后续处理",
   "id": "759246f57f705a29"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 加载并输出训练报告",
   "id": "f3d5cb023c517458"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "\n",
    "# 加载保存的数据\n",
    "with open(latest_train_data, \"rb\") as f:\n",
    "    training_data = pickle.load(f)\n",
    "\n",
    "loaded_reports = training_data['reports']\n",
    "\n",
    "for all_labels, all_predicts in loaded_reports:\n",
    "    print(classification_report(all_labels, all_predicts, zero_division=0, digits=4))"
   ],
   "id": "6d8c06e830fe2076"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 加载数据并绘图",
   "id": "8525a9f86841fb77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T09:17:22.645061Z",
     "start_time": "2025-02-14T09:17:22.633626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 创建一个简单的平滑函数\n",
    "def smooth(data, window_size=5):\n",
    "    window = np.ones(int(window_size)) / float(window_size)\n",
    "    return np.convolve(data, window, 'same')\n",
    "\n",
    "\n",
    "# 加载保存的数据\n",
    "with open(latest_train_data, \"rb\") as f:\n",
    "    training_data = pickle.load(f)\n",
    "\n",
    "train_losses = training_data['train_losses']\n",
    "valid_accs = training_data['valid_accs']\n",
    "\n",
    "# 应用平滑函数\n",
    "smoothed_train_losses = smooth(train_losses)\n",
    "smoothed_valid_accs = smooth(valid_accs)\n",
    "\n",
    "# 绘制平滑后的曲线\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(smoothed_train_losses, label=\"Train Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(smoothed_valid_accs, label=\"Validation Accuracy\")\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "fa99be728c4e0b87",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'latest_train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[104], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mconvolve(data, window, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msame\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# 加载保存的数据\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[43mlatest_train_data\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m     12\u001B[0m     training_data \u001B[38;5;241m=\u001B[39m pickle\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[1;32m     14\u001B[0m train_losses \u001B[38;5;241m=\u001B[39m training_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_losses\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'latest_train_data' is not defined"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train_losses\n",
    "valid_accs"
   ],
   "id": "c6dd80989d872d36",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
